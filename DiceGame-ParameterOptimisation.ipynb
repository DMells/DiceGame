{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promising-hundred",
   "metadata": {},
   "source": [
    "## Dice Game - Parameter Optimisation Using WandB\n",
    "### This notebook is for reference of parameter tuning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dice_game import DiceGame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DiceGameAgent(ABC):\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "    \n",
    "    @abstractmethod\n",
    "    def play(self, state):\n",
    "        pass\n",
    "\n",
    "def play_game_with_agent(agent, game, verbose=False):\n",
    "    state = game.reset()\n",
    "    \n",
    "    if(verbose): print(f\"Testing agent: \\n\\t{type(agent).__name__}\")\n",
    "    if(verbose): print(f\"Starting dice: \\n\\t{state}\\n\")\n",
    "    \n",
    "    game_over = False\n",
    "    actions = 0\n",
    "    while not game_over:\n",
    "        action = agent.play(state)\n",
    "        actions += 1\n",
    "        \n",
    "        if(verbose): print(f\"Action {actions}: \\t{action}\")\n",
    "        _, state, game_over = game.roll(action)\n",
    "        if(verbose and not game_over): print(f\"Dice: \\t\\t{state}\")\n",
    "\n",
    "    if(verbose): print(f\"\\nFinal dice: {state}, score: {game.score}\")\n",
    "        \n",
    "    return game.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgent(DiceGameAgent):\n",
    "    def __init__(self, game, theta, gamma):\n",
    "        \"\"\"\n",
    "        The initial constructor for the MyAgent class.\n",
    "        Upon intialisation calculates self.V, the dictionary containing all \n",
    "        possible states as keys and the best expected reward & optimal policy as values\n",
    "        \n",
    "        param/attribute game: the dice game object\n",
    "        \n",
    "        attribute gamma: the discount rate\n",
    "        attribute theta: the convergence threshold\n",
    "        attribute V: the state:[expected_reward, optimal policy] dictionary\n",
    "        \"\"\"\n",
    "        super().__init__(game)\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.num_actions = len(self.game.actions)\n",
    "        self.V = self.value_iteration()\n",
    "\n",
    "    \n",
    "    def one_step_look_ahead(self, V, state):\n",
    "        \"\"\"\n",
    "        Given a current state, iterates over every possible action and every possible next state to\n",
    "        calculate the Bellman update (the expected reward associated with choosing that action).\n",
    "        \n",
    "        The expected reward is the sum of the reward of the current state plus the expected reward of\n",
    "        the next state,\n",
    "        \n",
    "        param V: the (either converged or unconverged) dictionary of states: [expected rewards, optimal policy]\n",
    "        param state: the current state of the game\n",
    "        \n",
    "        returns A: an dictionary of actions containing the expected rewards of taking that action\n",
    "        \n",
    "        \"\"\"\n",
    "        A = {a:0 for a in self.game.actions}\n",
    "\n",
    "        # For each possible action\n",
    "        for a in A.keys():\n",
    "            \n",
    "            # Get the possible next_states, game_over, reward and probabilities of the next state occurring\n",
    "            next_states, game_over, reward, probabilities = self.game.get_next_states(a, state)\n",
    "            \n",
    "            # For each next_state and associated probability of that action & given state\n",
    "            for next_state, probability in zip(next_states, probabilities): \n",
    "                # If there is a next_state\n",
    "                if not game_over:\n",
    "                    # Calculate Bellman Update\n",
    "                    A[a] += probability * (reward+self.gamma*V[next_state][0])\n",
    "                else:\n",
    "                    # next_state = None\n",
    "                    A[a] += probability * reward\n",
    "        return A\n",
    "\n",
    "\n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        The core function to perform value iteration.\n",
    "        \n",
    "        Loops until the difference between the expected reward for each iteration and the \n",
    "        previous run-through for that state is less than self.theta i.e. convergence.\n",
    "        \n",
    "        returns V : the converged dictionary of states:[expected maximum reward, optimal policy]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialise dictionary with each state as keys and values [expected reward, optimal policy]\n",
    "        V = {i: [0, None] for i in self.game.states}\n",
    "        \n",
    "        # Loop until delta < self.theta\n",
    "        while True:\n",
    "            delta = 0\n",
    "    \n",
    "            # For each state\n",
    "            for s in self.game.states:\n",
    "                # Get the list of reward values pertaining to each action given that state\n",
    "                A = self.one_step_look_ahead(V, s)\n",
    "                \n",
    "                # Take the highest expected reward value \n",
    "                best_action_value = max(A.values())\n",
    "                \n",
    "                # Update delta with either delta or the absolute difference between the \n",
    "                # new maximum value for that state and the previous maximum value for that state\n",
    "                delta = max(delta, np.abs(best_action_value - V[s][0]))\n",
    "                \n",
    "                # Update the maximum value and optimal policy for that state   \n",
    "                V[s] = [best_action_value,max(A, key = A.get)]\n",
    "                \n",
    "            # If the change is less than self.theta, end the loop\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return V\n",
    "\n",
    "        \n",
    "    def play(self, state):\n",
    "        \"\"\"\n",
    "        For the given state, obtains the optimal policy\n",
    "        \n",
    "        param state : the current state of the game\n",
    "        \n",
    "        returns self.V[state][1] : the optimal action associated with that state\n",
    "        \"\"\"\n",
    "\n",
    "        return self.V[state][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and log in to wandb\n",
    "import wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config dict - parameters to iterate over, search method, and objective\n",
    "sweep_config = {\n",
    "  \"method\": \"grid\",\n",
    "  \"metric\": {\"name\": \"Avg Score\", \"goal\": \"maximise\"},\n",
    "  \"parameters\": {\n",
    "        \"gamma\": {\n",
    "                \"values\": [0.7, 0.8, 0.9, 0.99, 1]},\n",
    "      \"theta\": {\n",
    "            \"values\": [0.1, 1e-2, 1e-3, 1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10, 2e-2, 2e-3,2e-4,2e-5,2e-6,2e-7,2e-8,2e-9,2e-10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    The core train function that the wandb program will run iteratively for each parameter in the sweep config\n",
    "    \"\"\"\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    np.random.seed(10)\n",
    "    n = 10000\n",
    "\n",
    "    game = DiceGame()\n",
    "    total_score = 0\n",
    "    total_time = 0\n",
    "    start_time = time.process_time()\n",
    "    test_agent = MyAgent(game, config.theta, config.gamma)\n",
    "    total_time += time.process_time() - start_time\n",
    "    for i in range(n):\n",
    "        start_time = time.process_time()\n",
    "        score = play_game_with_agent(test_agent, game, verbose=False)\n",
    "        total_time += time.process_time() - start_time\n",
    "        total_score += score\n",
    "    wandb.log({\"Theta\": config.theta, \"Gamma\": config.gamma, \"Avg Score\": total_score/n, \"Avg Time\": total_time/n})\n",
    "    print(f\"Average score: {total_score/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sweep is the name of the iterative parameter selection process\n",
    "sweep_id = wandb.sweep(sweep_config, project='DiceGame')\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FoAI",
   "language": "python",
   "name": "foai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
